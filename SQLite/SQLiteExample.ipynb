{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite Tutorial: Ground Motion Database\n",
    "\n",
    "> Brandenberg, S.J. UCLA and Kumar, K. UT Austin\n",
    "\n",
    "## What is SQLite?\n",
    "\n",
    "**SQLite** is a lightweight, serverless, self-contained SQL database engine that stores data in a single file. It's perfect for:\n",
    "- **Small to medium applications** (websites, mobile apps, desktop software)\n",
    "- **Prototyping and development** (no server setup required)\n",
    "- **Data analysis and research** (portable databases)\n",
    "- **Embedded systems** (IoT devices, mobile applications)\n",
    "\n",
    "### Key Advantages of SQLite:\n",
    "- ✅ **Zero Configuration**: No server installation or management needed\n",
    "- ✅ **Cross-Platform**: Runs on Windows, macOS, Linux, Android, iOS\n",
    "- ✅ **ACID Compliant**: Atomic, Consistent, Isolated, Durable transactions\n",
    "- ✅ **Small Footprint**: Library is less than 1MB\n",
    "- ✅ **Fast**: Optimized for read operations and small datasets\n",
    "- ✅ **Reliable**: Extensively tested, used by millions of applications\n",
    "\n",
    "### When to Use SQLite vs Other Databases:\n",
    "\n",
    "| Feature | SQLite | PostgreSQL | MySQL |\n",
    "|---------|--------|------------|-------|\n",
    "| **Setup** | No setup required | Server installation | Server installation |\n",
    "| **Concurrent Writes** | Limited | Excellent | Good |\n",
    "| **Max Database Size** | 281 TB | Unlimited | Depends on storage |\n",
    "| **Use Case** | Development, small apps | Large applications | Web applications |\n",
    "| **Hosting** | File-based | Server required | Server required |\n",
    "\n",
    "---\n",
    "\n",
    "## Database Design Fundamentals\n",
    "\n",
    "This tutorial demonstrates a **relational database** for earthquake ground motion data with three interconnected tables:\n",
    "\n",
    "1. **`event`** - Earthquake event information\n",
    "2. **`station`** - Recording station details  \n",
    "3. **`motion`** - Ground motion recordings (links events and stations)\n",
    "\n",
    "### Database Schema Overview:\n",
    "```\n",
    "event (1) ←→ (many) motion (many) ←→ (1) station\n",
    "```\n",
    "\n",
    "This is a **many-to-many relationship** through the `motion` table:\n",
    "- One earthquake can be recorded at multiple stations\n",
    "- One station can record multiple earthquakes\n",
    "- Each recording has unique motion characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Important Warning\n",
    "\n",
    "**This notebook writes files to disk, so you must copy it to a writable location in DesignSafe before running.**\n",
    "\n",
    "### Steps to get started:\n",
    "1. **Copy this notebook** to your writable workspace (e.g., \"MyData\" folder)\n",
    "2. **Navigate to your copy** in the DesignSafe file browser\n",
    "3. **Open and run** the copied notebook\n",
    "\n",
    "**Why this is necessary:**\n",
    "- The notebook creates SQLite database files (.db files)\n",
    "- It exports data to CSV and Excel formats\n",
    "- The CommunityData directory is read-only\n",
    "- Your personal workspace (\"MyData\") has write permissions\n",
    "\n",
    "**How to copy the notebook:**\n",
    "- Right-click this notebook in the file browser → \"Copy\"\n",
    "- Navigate to your \"MyData\" folder → Right-click → \"Paste\"\n",
    "- Or use the DesignSafe file management interface to copy the file\n",
    "\n",
    "**Alternative save notebook as:**\n",
    "- You can save notebook as file  → \"Save Notebook As\" \n",
    "- In the popup window type \"MyData/SQLiteExample.ipynb\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72510ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages and restart the kernel\n",
    "%pip install \\\n",
    "  pandas>=1.5.0 \\\n",
    "  numpy>=1.21.0 \\\n",
    "  matplotlib>=3.5.0 \\\n",
    "  seaborn>=0.11.0 \\\n",
    "  jupyter>=1.0.0 \\\n",
    "  notebook>=6.4.0 \\\n",
    "  pyarrow>=10.0.0 \\\n",
    "  openpyxl>=3.0.0 \\\n",
    "  tables>=3.7.0 \\\n",
    "  h5py>=3.7.0 \\\n",
    "  plotly>=5.0.0 \\\n",
    "  bokeh>=2.4.0 \\\n",
    "  folium>=0.12.0 \\\n",
    "  scipy>=1.9.0 \\\n",
    "  scikit-learn>=1.1.0 \\\n",
    "  statsmodels>=0.13.0 \\\n",
    "  sqlalchemy>=1.4.0 \\\n",
    "  sqlite-utils>=3.30.0 \\\n",
    "  --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connecting to SQLite Database\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**`sqlite3.connect()`** creates or opens a database file:\n",
    "- If the file doesn't exist, SQLite creates it automatically\n",
    "- Returns a **Connection object** for database operations\n",
    "- **Best Practice**: Use context managers (`with` statement) for automatic cleanup\n",
    "\n",
    "**`connection.cursor()`** creates a cursor object:\n",
    "- **Cursor** = pointer that executes SQL commands and fetches results\n",
    "- Think of it as your \"command interface\" to the database\n",
    "- Multiple cursors can exist per connection\n",
    "\n",
    "### Error Handling in Database Operations:\n",
    "\n",
    "The `exec()` function below demonstrates proper error handling:\n",
    "- **`try/except`** blocks catch SQL errors (syntax, constraint violations, etc.)\n",
    "- **`sqlite3.Error`** is the base exception class for all SQLite errors\n",
    "- Always handle errors gracefully in production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16387ac-5abf-4780-80b0-2255620f45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "# Create a new database file called 'gmdatabase.db' and create a cursor\n",
    "con = sqlite3.connect('gmdatabase.db')\n",
    "cur = con.cursor()\n",
    "\n",
    "# Define a function that runs a query and returns an error if it fails\n",
    "def exec(cur, query):\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e0404",
   "metadata": {},
   "source": [
    "## Step 2: Creating Tables with SQL DDL\n",
    "\n",
    "### Understanding CREATE TABLE Syntax:\n",
    "\n",
    "**DDL (Data Definition Language)** defines database structure:\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS table_name (\n",
    "    column_name data_type constraints,\n",
    "    column_name data_type constraints,\n",
    "    ...\n",
    ");\n",
    "```\n",
    "\n",
    "### SQLite Data Types:\n",
    "- **`INTEGER`** - Whole numbers (1, 42, -17)\n",
    "- **`REAL`** - Floating point numbers (3.14, -0.5, 1.2e5)\n",
    "- **`TEXT`** - Strings (\"earthquake\", 'station name')\n",
    "- **`BLOB`** - Binary data (images, files)\n",
    "- **`NULL`** - Missing/unknown values\n",
    "\n",
    "### Key Constraints Explained:\n",
    "\n",
    "1. **`PRIMARY KEY`** - Unique identifier for each row\n",
    "   - Automatically creates an index for fast lookups\n",
    "   - Cannot be NULL or duplicate\n",
    "   - Use `INTEGER PRIMARY KEY` for auto-incrementing IDs\n",
    "\n",
    "2. **`FOREIGN KEY`** - Links tables together\n",
    "   - References the primary key of another table\n",
    "   - Maintains **referential integrity** (prevents orphaned records)\n",
    "   - Example: `motion.event_id` must exist in `event.event_id`\n",
    "\n",
    "3. **`IF NOT EXISTS`** - Safe table creation\n",
    "   - Only creates table if it doesn't already exist\n",
    "   - Prevents errors when running scripts multiple times\n",
    "\n",
    "### Database Schema Analysis:\n",
    "\n",
    "Our three tables demonstrate a classic **many-to-many relationship**:\n",
    "- **`event`**: Stores earthquake information (magnitude, location)\n",
    "- **`station`**: Stores recording station details (location, soil properties)  \n",
    "- **`motion`**: Junction table linking events to stations with measurement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c159d85-7465-48cf-a5cc-85a86f55dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables. Note that the motion table has two foreign keys: event_id and station_id\n",
    "\n",
    "query = 'CREATE TABLE IF NOT EXISTS event ( '\n",
    "query += 'event_id integer primary key, '\n",
    "query += 'event_name text, '\n",
    "query += 'magnitude real, '\n",
    "query += 'epicentral_latitude real, '\n",
    "query += 'epicentral_longitude real)'\n",
    "exec(cur, query)\n",
    "    \n",
    "query = 'CREATE TABLE IF NOT EXISTS station ( '\n",
    "query += 'station_id integer primary key, '\n",
    "query += 'station_name text, '\n",
    "query += 'station_latitude real, '\n",
    "query += 'station_longitude real, '\n",
    "query += 'vs30 real)'\n",
    "exec(cur, query)\n",
    "    \n",
    "query = 'CREATE TABLE IF NOT EXISTS motion ( '\n",
    "query += 'motion_id integer primary key, '\n",
    "query += 'event_id INTEGER, '\n",
    "query += 'station_id INTEGER, '\n",
    "query += 'rjb real, '\n",
    "query += 'pga real, '\n",
    "query += 'FOREIGN KEY(event_id) REFERENCES event(event_id), '\n",
    "query += 'FOREIGN KEY(station_id) REFERENCES station(station_id))'\n",
    "exec(cur, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ba8a3",
   "metadata": {},
   "source": [
    "## Step 3: Inserting Data with SQL DML\n",
    "\n",
    "### Understanding INSERT Syntax:\n",
    "\n",
    "**DML (Data Manipulation Language)** modifies database content:\n",
    "```sql\n",
    "INSERT INTO table_name (column1, column2, ...) \n",
    "VALUES (value1, value2, ...);\n",
    "```\n",
    "\n",
    "### Best Practices for Data Insertion:\n",
    "\n",
    "1. **Explicit Column Names**: Always specify column names for clarity\n",
    "   - `INSERT INTO event (event_id, event_name, ...)` ✅\n",
    "   - `INSERT INTO event VALUES (1, \"name\", ...)` ❌ (fragile)\n",
    "\n",
    "2. **Data Type Matching**: Ensure values match column types\n",
    "   - `magnitude real` expects numbers: `6.3` ✅, `\"6.3\"` works but not ideal\n",
    "   - `event_name text` expects strings: `\"Westwood Hills\"` ✅\n",
    "\n",
    "3. **Primary Key Management**: \n",
    "   - We're manually setting IDs here for simplicity\n",
    "   - In production, often use `INTEGER PRIMARY KEY` for auto-increment\n",
    "   - SQLite automatically handles ID assignment if you omit the ID column\n",
    "\n",
    "### Data Validation Tips:\n",
    "- **Coordinates**: Latitude (-90 to 90), Longitude (-180 to 180)\n",
    "- **Magnitude**: Typically 0-10 scale (Richter/Moment magnitude)\n",
    "- **PGA (Peak Ground Acceleration)**: Usually 0-2g for most earthquakes\n",
    "- **Vs30**: Soil shear wave velocity, typically 150-1500 m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23af57be-49bd-4dcf-b864-9dfe5977912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE constraint failed: event.event_id\n"
     ]
    }
   ],
   "source": [
    "# Insert data into event table\n",
    "\n",
    "query = 'INSERT INTO event (event_id, event_name, magnitude, epicentral_latitude, epicentral_longitude) VALUES (1, \"Westwood Hills\", 6.3, 34.0689, -118.4452)'\n",
    "exec(cur, query)\n",
    "query = 'INSERT INTO event (event_id, event_name, magnitude, epicentral_latitude, epicentral_longitude) VALUES (2, \"Hollywood Valley\", 7.2, 34.1027, -118.3404)'\n",
    "exec(cur, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20684fdf",
   "metadata": {},
   "source": [
    "## Step 4: Querying Data with SELECT\n",
    "\n",
    "### Understanding SELECT Syntax:\n",
    "\n",
    "```sql\n",
    "SELECT column1, column2, ... \n",
    "FROM table_name \n",
    "WHERE condition;\n",
    "```\n",
    "\n",
    "**`SELECT *`** retrieves all columns from a table:\n",
    "- Useful for exploration and small datasets\n",
    "- In production, specify exact columns for better performance\n",
    "- Example: `SELECT event_name, magnitude FROM event`\n",
    "\n",
    "**`cur.fetchall()`** retrieves all query results:\n",
    "- Returns a list of tuples, one per row\n",
    "- Alternative methods: `fetchone()` (single row), `fetchmany(n)` (n rows)\n",
    "- Always call fetch methods after executing SELECT queries\n",
    "\n",
    "### Query Optimization Tips:\n",
    "- **Use indexes**: Primary keys are automatically indexed\n",
    "- **Limit results**: Add `LIMIT 100` for large datasets\n",
    "- **Filter early**: Use `WHERE` clauses to reduce data transfer\n",
    "- **Select specific columns**: Avoid `SELECT *` in production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e92d98b-946e-49f4-a745-42179db64055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'San Andreas Test', 5.8, 34.0522, -118.2437),\n",
       " (2, 'Hollywood Valley', 7.2, 34.1027, -118.3404)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query event table to make sure the data are entered correctly\n",
    "\n",
    "query = 'SELECT * FROM event'\n",
    "exec(cur, query)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e765f9-2bb9-4c74-ba27-a770569000dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into station table\n",
    "\n",
    "query = 'INSERT INTO station (station_id, station_name, station_latitude, station_longitude, vs30) VALUES (1, \"Factor Building\", 34.006677, -118.442007, 450)'\n",
    "exec(cur, query)\n",
    "query = 'INSERT INTO station (station_id, station_name, station_latitude, station_longitude, vs30) VALUES (2, \"Santa Monica Courthouse\", 34.0108, -118.4903, 275)'\n",
    "exec(cur, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0902a8",
   "metadata": {},
   "source": [
    "## Step 5: Inserting Station Data\n",
    "\n",
    "### Understanding Geospatial Data:\n",
    "\n",
    "**Station Location Data**:\n",
    "- **Latitude/Longitude**: Geographic coordinates in decimal degrees\n",
    "- **Vs30**: Average shear wave velocity in top 30 meters of soil\n",
    "  - Important for ground motion prediction\n",
    "  - Higher Vs30 = stiffer soil = typically lower amplification\n",
    "\n",
    "**Real-World Context**:\n",
    "- Factor Building: Research facility near UCLA\n",
    "- Santa Monica Courthouse: Government building in Santa Monica\n",
    "- Both locations represent different soil conditions (Vs30 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93e19a2-f557-4c9b-9e76-471e3813a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into motion table\n",
    "\n",
    "query = 'INSERT INTO motion (motion_id, event_id, station_id, rjb, pga) VALUES (1, 1, 1, 2.0, 0.84)'\n",
    "exec(cur, query)\n",
    "query = 'INSERT INTO motion (motion_id, event_id, station_id, rjb, pga) VALUES (2, 1, 2, 14.0, 0.28)'\n",
    "exec(cur, query)\n",
    "query = 'INSERT INTO motion (motion_id, event_id, station_id, rjb, pga) VALUES (3, 2, 1, 20.0, 0.61)'\n",
    "exec(cur, query)\n",
    "query = 'INSERT INTO motion (motion_id, event_id, station_id, rjb, pga) VALUES (4, 2, 2, 30.0, 0.32)'\n",
    "exec(cur, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7679a6",
   "metadata": {},
   "source": [
    "## Step 6: Recording Ground Motion Data\n",
    "\n",
    "### Understanding Ground Motion Parameters:\n",
    "\n",
    "**RJB (Joyner-Boore Distance)**:\n",
    "- Distance from recording site to closest point on fault rupture surface\n",
    "- Critical parameter for ground motion prediction equations\n",
    "- Measured in kilometers\n",
    "\n",
    "**PGA (Peak Ground Acceleration)**:\n",
    "- Maximum acceleration recorded during earthquake\n",
    "- Expressed as fraction of gravity (g = 9.81 m/s²)\n",
    "- Key parameter for structural engineering and seismic hazard\n",
    "\n",
    "### Data Relationships:\n",
    "Each motion record connects:\n",
    "1. **One earthquake event** (via event_id foreign key)\n",
    "2. **One recording station** (via station_id foreign key)\n",
    "3. **Unique ground motion measurements** for that event-station pair\n",
    "\n",
    "This creates a complete record of \"what was recorded where during which earthquake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee81de73-c8cf-4bc0-aeaa-0f72ccc23f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  1,\n",
       "  1,\n",
       "  2.0,\n",
       "  0.84,\n",
       "  1,\n",
       "  'Factor Building',\n",
       "  34.006677,\n",
       "  -118.442007,\n",
       "  450.0,\n",
       "  1,\n",
       "  'San Andreas Test',\n",
       "  5.8,\n",
       "  34.0522,\n",
       "  -118.2437),\n",
       " (2,\n",
       "  1,\n",
       "  2,\n",
       "  14.0,\n",
       "  0.28,\n",
       "  2,\n",
       "  'Santa Monica Courthouse',\n",
       "  34.0108,\n",
       "  -118.4903,\n",
       "  275.0,\n",
       "  1,\n",
       "  'San Andreas Test',\n",
       "  5.8,\n",
       "  34.0522,\n",
       "  -118.2437),\n",
       " (3,\n",
       "  2,\n",
       "  1,\n",
       "  20.0,\n",
       "  0.61,\n",
       "  1,\n",
       "  'Factor Building',\n",
       "  34.006677,\n",
       "  -118.442007,\n",
       "  450.0,\n",
       "  2,\n",
       "  'Hollywood Valley',\n",
       "  7.2,\n",
       "  34.1027,\n",
       "  -118.3404),\n",
       " (4,\n",
       "  2,\n",
       "  2,\n",
       "  30.0,\n",
       "  0.32,\n",
       "  2,\n",
       "  'Santa Monica Courthouse',\n",
       "  34.0108,\n",
       "  -118.4903,\n",
       "  275.0,\n",
       "  2,\n",
       "  'Hollywood Valley',\n",
       "  7.2,\n",
       "  34.1027,\n",
       "  -118.3404)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a query that joins the tables on primary / foreign key constraints\n",
    "\n",
    "query = 'SELECT * FROM motion JOIN station ON motion.station_id = station.station_id JOIN event ON motion.event_id = event.event_id'\n",
    "exec(cur, query)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08695453",
   "metadata": {},
   "source": [
    "## Step 7: Advanced Querying with JOINs\n",
    "\n",
    "### Understanding JOIN Operations:\n",
    "\n",
    "**JOIN** combines data from multiple tables based on related columns:\n",
    "```sql\n",
    "SELECT columns\n",
    "FROM table1 \n",
    "JOIN table2 ON table1.key = table2.key\n",
    "JOIN table3 ON table1.key = table3.key;\n",
    "```\n",
    "\n",
    "### Types of JOINs:\n",
    "- **INNER JOIN** (default): Returns only matching records from both tables\n",
    "- **LEFT JOIN**: Returns all records from left table + matching from right\n",
    "- **RIGHT JOIN**: Returns all records from right table + matching from left  \n",
    "- **FULL OUTER JOIN**: Returns all records from both tables\n",
    "\n",
    "### Our Query Breakdown:\n",
    "```sql\n",
    "SELECT * FROM motion \n",
    "JOIN station ON motion.station_id = station.station_id \n",
    "JOIN event ON motion.event_id = event.event_id\n",
    "```\n",
    "\n",
    "This creates a **complete picture** by combining:\n",
    "1. **Motion measurements** (PGA, distance)\n",
    "2. **Station details** (location, soil properties)  \n",
    "3. **Event information** (magnitude, epicenter)\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Seismic hazard analysis**: Compare ground motion vs distance and magnitude\n",
    "- **Site response studies**: Analyze how different soil types (Vs30) affect shaking\n",
    "- **Building code development**: Understand ground motion patterns for design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3b75d",
   "metadata": {},
   "source": [
    "## Key Takeaways and Next Steps\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **SQLite Basics**: Database connection, cursor operations, error handling\n",
    "2. **Database Design**: Primary keys, foreign keys, relational structure\n",
    "3. **SQL Operations**: CREATE, INSERT, SELECT, JOIN\n",
    "4. **Data Relationships**: Many-to-many relationships through junction tables\n",
    "5. **Real-World Application**: Ground motion database for earthquake engineering\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "\n",
    "✅ **Error Handling**: Try/except blocks for robust database operations  \n",
    "✅ **Safe Table Creation**: Using `IF NOT EXISTS` to prevent conflicts  \n",
    "✅ **Referential Integrity**: Foreign key constraints maintain data consistency  \n",
    "✅ **Explicit Column Naming**: Clear, maintainable INSERT statements  \n",
    "✅ **Meaningful JOINs**: Combining related data for comprehensive analysis  \n",
    "\n",
    "### Next Steps for Advanced Usage:\n",
    "\n",
    "1. **Indexes**: Add `CREATE INDEX` for faster queries on large datasets\n",
    "2. **Transactions**: Use `BEGIN/COMMIT` for multi-statement operations\n",
    "3. **Parameterized Queries**: Prevent SQL injection with `?` placeholders\n",
    "4. **Database Migration**: Version control your schema changes\n",
    "5. **Integration**: Connect with pandas, matplotlib for data analysis\n",
    "\n",
    "### Performance Optimization:\n",
    "\n",
    "```sql\n",
    "-- Add indexes for frequently queried columns\n",
    "CREATE INDEX idx_motion_event ON motion(event_id);\n",
    "CREATE INDEX idx_motion_station ON motion(station_id);\n",
    "\n",
    "-- Use parameterized queries for safety\n",
    "cursor.execute(\"SELECT * FROM event WHERE magnitude > ?\", (min_mag,))\n",
    "\n",
    "-- Use transactions for bulk operations\n",
    "con.execute(\"BEGIN\")\n",
    "# Multiple INSERT operations\n",
    "con.execute(\"COMMIT\")\n",
    "```\n",
    "\n",
    "### Production Recommendations:\n",
    "\n",
    "- **Use context managers**: `with sqlite3.connect() as con:`\n",
    "- **Validate input data**: Check ranges, data types before insertion\n",
    "- **Regular backups**: SQLite files are easy to backup/restore\n",
    "- **Monitor file size**: Consider partitioning for very large datasets\n",
    "- **Test thoroughly**: Validate foreign key relationships and constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb5cee",
   "metadata": {},
   "source": [
    "## Advanced SQL Examples and Data Export/Import\n",
    "\n",
    "### More Complex Queries with Aggregation and Filtering\n",
    "\n",
    "Let's explore advanced SQL operations with our ground motion database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7356f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average ground motion by earthquake event:\n",
      "==================================================\n",
      "Event: Hollywood Valley\n",
      "  Magnitude: 7.2\n",
      "  Recordings: 2\n",
      "  Avg PGA: 0.465g\n",
      "  Max PGA: 0.610g\n",
      "  Closest Station: 20.0km\n",
      "\n",
      "Event: San Andreas Test\n",
      "  Magnitude: 5.8\n",
      "  Recordings: 2\n",
      "  Avg PGA: 0.560g\n",
      "  Max PGA: 0.840g\n",
      "  Closest Station: 2.0km\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Find average PGA by earthquake event\n",
    "query = '''\n",
    "SELECT \n",
    "    e.event_name,\n",
    "    e.magnitude,\n",
    "    COUNT(m.motion_id) as num_recordings,\n",
    "    AVG(m.pga) as avg_pga,\n",
    "    MAX(m.pga) as max_pga,\n",
    "    MIN(m.rjb) as closest_distance\n",
    "FROM event e\n",
    "JOIN motion m ON e.event_id = m.event_id\n",
    "GROUP BY e.event_id, e.event_name, e.magnitude\n",
    "ORDER BY e.magnitude DESC\n",
    "'''\n",
    "\n",
    "print(\"📊 Average ground motion by earthquake event:\")\n",
    "print(\"=\" * 50)\n",
    "exec(cur, query)\n",
    "results = cur.fetchall()\n",
    "for row in results:\n",
    "    print(f\"Event: {row[0]}\")\n",
    "    print(f\"  Magnitude: {row[1]}\")\n",
    "    print(f\"  Recordings: {row[2]}\")\n",
    "    print(f\"  Avg PGA: {row[3]:.3f}g\")\n",
    "    print(f\"  Max PGA: {row[4]:.3f}g\") \n",
    "    print(f\"  Closest Station: {row[5]:.1f}km\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f97c33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 High ground motion recordings (PGA > 0.5g):\n",
      "=======================================================\n",
      "Event: San Andreas Test | Station: Factor Building\n",
      "  PGA: 0.840g | Distance: 2.0km\n",
      "  Vs30: 450m/s (Medium Soil)\n",
      "\n",
      "Event: Hollywood Valley | Station: Factor Building\n",
      "  PGA: 0.610g | Distance: 20.0km\n",
      "  Vs30: 450m/s (Medium Soil)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Find high ground motion recordings (PGA > 0.5g)\n",
    "query = '''\n",
    "SELECT \n",
    "    e.event_name,\n",
    "    s.station_name,\n",
    "    m.pga,\n",
    "    m.rjb,\n",
    "    s.vs30,\n",
    "    CASE \n",
    "        WHEN s.vs30 < 300 THEN 'Soft Soil'\n",
    "        WHEN s.vs30 < 600 THEN 'Medium Soil' \n",
    "        ELSE 'Hard Soil'\n",
    "    END as soil_type\n",
    "FROM motion m\n",
    "JOIN event e ON m.event_id = e.event_id\n",
    "JOIN station s ON m.station_id = s.station_id\n",
    "WHERE m.pga > 0.5\n",
    "ORDER BY m.pga DESC\n",
    "'''\n",
    "\n",
    "print(\"🚨 High ground motion recordings (PGA > 0.5g):\")\n",
    "print(\"=\" * 55)\n",
    "exec(cur, query)\n",
    "high_motion = cur.fetchall()\n",
    "for row in high_motion:\n",
    "    print(f\"Event: {row[0]} | Station: {row[1]}\")\n",
    "    print(f\"  PGA: {row[2]:.3f}g | Distance: {row[3]:.1f}km\")\n",
    "    print(f\"  Vs30: {row[4]:.0f}m/s ({row[5]})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424e86c",
   "metadata": {},
   "source": [
    "## Integrating SQLite with Pandas\n",
    "\n",
    "### Why Use Pandas with SQLite?\n",
    "\n",
    "**Pandas** provides powerful data analysis capabilities that complement SQLite:\n",
    "- **Data manipulation**: Advanced filtering, grouping, and transformations\n",
    "- **Visualization**: Easy plotting with matplotlib/seaborn integration\n",
    "- **Export options**: Multiple file formats (CSV, Excel, JSON, Parquet)\n",
    "- **Statistical analysis**: Built-in descriptive statistics and correlation analysis\n",
    "\n",
    "Let's demonstrate the integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112b0f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Loading data into pandas DataFrames:\n",
      "========================================\n",
      "Events table: 2 rows\n",
      "   event_id        event_name  magnitude  epicentral_latitude  \\\n",
      "0         1  San Andreas Test        5.8              34.0522   \n",
      "1         2  Hollywood Valley        7.2              34.1027   \n",
      "\n",
      "   epicentral_longitude  \n",
      "0             -118.2437  \n",
      "1             -118.3404  \n",
      "\n",
      "Stations table: 2 rows\n",
      "   station_id             station_name  station_latitude  station_longitude  \\\n",
      "0           1          Factor Building         34.006677        -118.442007   \n",
      "1           2  Santa Monica Courthouse         34.010800        -118.490300   \n",
      "\n",
      "    vs30  \n",
      "0  450.0  \n",
      "1  275.0  \n",
      "\n",
      "Motion table: 4 rows\n",
      "   motion_id  event_id  station_id   rjb   pga\n",
      "0          1         1           1   2.0  0.84\n",
      "1          2         1           2  14.0  0.28\n",
      "2          3         2           1  20.0  0.61\n",
      "3          4         2           2  30.0  0.32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Method 1: Read entire tables into pandas DataFrames\n",
    "print(\"📋 Loading data into pandas DataFrames:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load each table\n",
    "events_df = pd.read_sql_query(\"SELECT * FROM event\", con)\n",
    "stations_df = pd.read_sql_query(\"SELECT * FROM station\", con)\n",
    "motion_df = pd.read_sql_query(\"SELECT * FROM motion\", con)\n",
    "\n",
    "print(f\"Events table: {len(events_df)} rows\")\n",
    "print(events_df.head())\n",
    "print(f\"\\nStations table: {len(stations_df)} rows\") \n",
    "print(stations_df.head())\n",
    "print(f\"\\nMotion table: {len(motion_df)} rows\")\n",
    "print(motion_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69da6777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Combined ground motion dataset:\n",
      "===================================\n",
      "         event_name  magnitude             station_name   vs30  distance_km  \\\n",
      "0  Hollywood Valley        7.2          Factor Building  450.0         20.0   \n",
      "1  Hollywood Valley        7.2  Santa Monica Courthouse  275.0         30.0   \n",
      "2  San Andreas Test        5.8          Factor Building  450.0          2.0   \n",
      "3  San Andreas Test        5.8  Santa Monica Courthouse  275.0         14.0   \n",
      "\n",
      "    pga soil_class  \n",
      "0  0.61     Medium  \n",
      "1  0.32       Soft  \n",
      "2  0.84     Medium  \n",
      "3  0.28       Soft  \n",
      "\n",
      "Dataset shape: (4, 7)\n",
      "Columns: ['event_name', 'magnitude', 'station_name', 'vs30', 'distance_km', 'pga', 'soil_class']\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Complex query directly to pandas DataFrame\n",
    "complex_query = '''\n",
    "SELECT \n",
    "    e.event_name,\n",
    "    e.magnitude,\n",
    "    s.station_name,\n",
    "    s.vs30,\n",
    "    m.rjb as distance_km,\n",
    "    m.pga,\n",
    "    CASE \n",
    "        WHEN s.vs30 < 300 THEN 'Soft'\n",
    "        WHEN s.vs30 < 600 THEN 'Medium' \n",
    "        ELSE 'Hard'\n",
    "    END as soil_class\n",
    "FROM motion m\n",
    "JOIN event e ON m.event_id = e.event_id\n",
    "JOIN station s ON m.station_id = s.station_id\n",
    "ORDER BY e.magnitude DESC, m.rjb ASC\n",
    "'''\n",
    "\n",
    "# Load complex joined data directly into pandas\n",
    "ground_motion_df = pd.read_sql_query(complex_query, con)\n",
    "\n",
    "print(\"🔗 Combined ground motion dataset:\")\n",
    "print(\"=\" * 35)\n",
    "print(ground_motion_df)\n",
    "print(f\"\\nDataset shape: {ground_motion_df.shape}\")\n",
    "print(f\"Columns: {list(ground_motion_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce0e2d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Data Analysis with Pandas:\n",
      "==============================\n",
      "📊 Descriptive Statistics:\n",
      "       magnitude  distance_km       pga        vs30\n",
      "count    4.00000       4.0000  4.000000    4.000000\n",
      "mean     6.50000      16.5000  0.512500  362.500000\n",
      "std      0.80829      11.7047  0.263233  101.036297\n",
      "min      5.80000       2.0000  0.280000  275.000000\n",
      "25%      5.80000      11.0000  0.310000  275.000000\n",
      "50%      6.50000      17.0000  0.465000  362.500000\n",
      "75%      7.20000      22.5000  0.667500  450.000000\n",
      "max      7.20000      30.0000  0.840000  450.000000\n",
      "\n",
      "🏗️ Ground Motion by Soil Type:\n",
      "              pga                vs30 distance_km\n",
      "             mean    std   max   mean        mean\n",
      "soil_class                                       \n",
      "Medium      0.725  0.163  0.84  450.0        11.0\n",
      "Soft        0.300  0.028  0.32  275.0        22.0\n",
      "\n",
      "🎯 Correlation Analysis:\n",
      "             magnitude  distance_km    pga   vs30\n",
      "magnitude        1.000        0.839 -0.208  0.000\n",
      "distance_km      0.839        1.000 -0.695 -0.543\n",
      "pga             -0.208       -0.695  1.000  0.932\n",
      "vs30             0.000       -0.543  0.932  1.000\n"
     ]
    }
   ],
   "source": [
    "# Pandas data analysis examples\n",
    "print(\"📈 Data Analysis with Pandas:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"📊 Descriptive Statistics:\")\n",
    "print(ground_motion_df[['magnitude', 'distance_km', 'pga', 'vs30']].describe())\n",
    "\n",
    "print(\"\\n🏗️ Ground Motion by Soil Type:\")\n",
    "soil_analysis = ground_motion_df.groupby('soil_class').agg({\n",
    "    'pga': ['mean', 'std', 'max'],\n",
    "    'vs30': 'mean',\n",
    "    'distance_km': 'mean'\n",
    "}).round(3)\n",
    "print(soil_analysis)\n",
    "\n",
    "print(\"\\n🎯 Correlation Analysis:\")\n",
    "correlations = ground_motion_df[['magnitude', 'distance_km', 'pga', 'vs30']].corr()\n",
    "print(correlations.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427865ed",
   "metadata": {},
   "source": [
    "## Data Export: From SQLite to Various Formats\n",
    "\n",
    "### CSV Export (Most Common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3b4cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Exporting data to CSV format:\n",
      "===================================\n",
      "✅ CSV files created:\n",
      "  - earthquake_events.csv\n",
      "  - seismic_stations.csv\n",
      "  - ground_motion_data.csv\n",
      "  - combined_ground_motion.csv\n",
      "\n",
      "📄 Preview of combined_ground_motion.csv:\n",
      "  event_name,magnitude,station_name,vs30,distance_km,pga,soil_class\n",
      "  Hollywood Valley,7.2,Factor Building,450.0,20.0,0.61,Medium\n",
      "  Hollywood Valley,7.2,Santa Monica Courthouse,275.0,30.0,0.32,Soft\n",
      "  San Andreas Test,5.8,Factor Building,450.0,2.0,0.84,Medium\n",
      "  San Andreas Test,5.8,Santa Monica Courthouse,275.0,14.0,0.28,Soft\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV files\n",
    "print(\"💾 Exporting data to CSV format:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Export individual tables\n",
    "events_df.to_csv('earthquake_events.csv', index=False)\n",
    "stations_df.to_csv('seismic_stations.csv', index=False)\n",
    "motion_df.to_csv('ground_motion_data.csv', index=False)\n",
    "\n",
    "# Export combined dataset\n",
    "ground_motion_df.to_csv('combined_ground_motion.csv', index=False)\n",
    "\n",
    "print(\"✅ CSV files created:\")\n",
    "print(\"  - earthquake_events.csv\")\n",
    "print(\"  - seismic_stations.csv\") \n",
    "print(\"  - ground_motion_data.csv\")\n",
    "print(\"  - combined_ground_motion.csv\")\n",
    "\n",
    "# Show CSV content preview\n",
    "print(f\"\\n📄 Preview of combined_ground_motion.csv:\")\n",
    "with open('combined_ground_motion.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        print(f\"  {line.strip()}\")\n",
    "        if i >= 4:  # Show first 5 lines\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c999bad",
   "metadata": {},
   "source": [
    "### Excel Export (Multiple Sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3028b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Exporting data to Excel format:\n",
      "===================================\n",
      "✅ Excel file created: ground_motion_database.xlsx\n",
      "   📋 Sheets included:\n",
      "     - Events (earthquake data)\n",
      "     - Stations (recording stations)\n",
      "     - GroundMotion (raw measurements)\n",
      "     - Combined (joined dataset)\n",
      "     - Summary (data overview)\n"
     ]
    }
   ],
   "source": [
    "# Export to Excel with multiple sheets\n",
    "print(\"📊 Exporting data to Excel format:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Create Excel file with multiple sheets\n",
    "    with pd.ExcelWriter('ground_motion_database.xlsx', engine='openpyxl') as writer:\n",
    "        # Write each table to a separate sheet\n",
    "        events_df.to_excel(writer, sheet_name='Events', index=False)\n",
    "        stations_df.to_excel(writer, sheet_name='Stations', index=False)\n",
    "        motion_df.to_excel(writer, sheet_name='GroundMotion', index=False)\n",
    "        ground_motion_df.to_excel(writer, sheet_name='Combined', index=False)\n",
    "        \n",
    "        # Create a summary sheet\n",
    "        summary_data = {\n",
    "            'Table': ['Events', 'Stations', 'Ground Motion', 'Combined Dataset'],\n",
    "            'Rows': [len(events_df), len(stations_df), len(motion_df), len(ground_motion_df)],\n",
    "            'Columns': [len(events_df.columns), len(stations_df.columns), \n",
    "                       len(motion_df.columns), len(ground_motion_df.columns)]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    print(\"✅ Excel file created: ground_motion_database.xlsx\")\n",
    "    print(\"   📋 Sheets included:\")\n",
    "    print(\"     - Events (earthquake data)\")\n",
    "    print(\"     - Stations (recording stations)\")\n",
    "    print(\"     - GroundMotion (raw measurements)\")\n",
    "    print(\"     - Combined (joined dataset)\")\n",
    "    print(\"     - Summary (data overview)\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"❌ Excel export requires openpyxl library\")\n",
    "    print(\"   Install with: pip install openpyxl\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Excel export failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae1a56",
   "metadata": {},
   "source": [
    "## Data Import: From Files Back to SQLite\n",
    "\n",
    "### Reading CSV Files and Importing to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac3453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Importing CSV data back to SQLite:\n",
      "======================================\n",
      "✅ Read CSV file: 4 rows\n",
      "✅ Data imported to new database: 4 rows\n",
      "\n",
      "📋 Imported table schema:\n",
      "  event_name (TEXT)\n",
      "  magnitude (REAL)\n",
      "  station_name (TEXT)\n",
      "  vs30 (REAL)\n",
      "  distance_km (REAL)\n",
      "  pga (REAL)\n",
      "  soil_class (TEXT)\n",
      "\n",
      "📄 Sample imported data:\n",
      "  Hollywood Valley | Factor Building | PGA: 0.61\n",
      "  Hollywood Valley | Santa Monica Courthouse | PGA: 0.32\n",
      "  San Andreas Test | Factor Building | PGA: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate importing CSV data back to SQLite\n",
    "print(\"📥 Importing CSV data back to SQLite:\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "# Create a new database for the import demo\n",
    "import_con = sqlite3.connect('imported_gmdatabase.db')\n",
    "import_cur = import_con.cursor()\n",
    "\n",
    "# Read CSV file and import to new database\n",
    "try:\n",
    "    # Read CSV file\n",
    "    imported_df = pd.read_csv('combined_ground_motion.csv')\n",
    "    print(f\"✅ Read CSV file: {len(imported_df)} rows\")\n",
    "    \n",
    "    # Write DataFrame to SQLite table\n",
    "    imported_df.to_sql('ground_motion_analysis', import_con, \n",
    "                      if_exists='replace', index=False)\n",
    "    \n",
    "    # Verify the import\n",
    "    verify_query = \"SELECT COUNT(*) FROM ground_motion_analysis\"\n",
    "    import_cur.execute(verify_query)\n",
    "    row_count = import_cur.fetchone()[0]\n",
    "    \n",
    "    print(f\"✅ Data imported to new database: {row_count} rows\")\n",
    "    \n",
    "    # Show table schema\n",
    "    schema_query = \"PRAGMA table_info(ground_motion_analysis)\"\n",
    "    import_cur.execute(schema_query)\n",
    "    schema_info = import_cur.fetchall()\n",
    "    \n",
    "    print(\"\\n📋 Imported table schema:\")\n",
    "    for col_info in schema_info:\n",
    "        print(f\"  {col_info[1]} ({col_info[2]})\")\n",
    "    \n",
    "    # Test query on imported data\n",
    "    test_query = \"SELECT event_name, station_name, pga FROM ground_motion_analysis LIMIT 3\"\n",
    "    import_cur.execute(test_query)\n",
    "    sample_data = import_cur.fetchall()\n",
    "    \n",
    "    print(\"\\n📄 Sample imported data:\")\n",
    "    for row in sample_data:\n",
    "        print(f\"  {row[0]} | {row[1]} | PGA: {row[2]}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ CSV file not found. Run the export section first.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "finally:\n",
    "    import_con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6ebbe",
   "metadata": {},
   "source": [
    "### Reading Excel Files and Advanced Import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7518979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Reading Excel files and importing to SQLite:\n",
      "===============================================\n",
      "✅ Excel file contains 5 sheets:\n",
      "  📋 Events: 2 rows\n",
      "  📋 Stations: 2 rows\n",
      "  📋 GroundMotion: 4 rows\n",
      "  📋 Combined: 4 rows\n",
      "  📋 Summary: 4 rows\n",
      "\n",
      "✅ Read 'Events' sheet: 2 rows\n",
      "First few rows:\n",
      "   event_id        event_name  magnitude  epicentral_latitude  \\\n",
      "0         1  San Andreas Test        5.8              34.0522   \n",
      "1         2  Hollywood Valley        7.2              34.1027   \n",
      "\n",
      "   epicentral_longitude  \n",
      "0             -118.2437  \n",
      "1             -118.3404  \n",
      "✅ Imported Events → excel_events table\n",
      "✅ Imported Stations → excel_stations table\n",
      "✅ Imported GroundMotion → excel_groundmotion table\n",
      "✅ Imported Combined → excel_combined table\n",
      "\n",
      "🎉 Excel import completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reading Excel files and importing to SQLite\n",
    "print(\"📊 Reading Excel files and importing to SQLite:\")\n",
    "print(\"=\" * 47)\n",
    "\n",
    "try:\n",
    "    # Read specific sheets from Excel file\n",
    "    excel_file = 'ground_motion_database.xlsx'\n",
    "    \n",
    "    # Method 1: Read all sheets\n",
    "    all_sheets = pd.read_excel(excel_file, sheet_name=None)\n",
    "    print(f\"✅ Excel file contains {len(all_sheets)} sheets:\")\n",
    "    for sheet_name in all_sheets.keys():\n",
    "        rows = len(all_sheets[sheet_name])\n",
    "        print(f\"  📋 {sheet_name}: {rows} rows\")\n",
    "    \n",
    "    # Method 2: Read specific sheet\n",
    "    events_from_excel = pd.read_excel(excel_file, sheet_name='Events')\n",
    "    print(f\"\\n✅ Read 'Events' sheet: {len(events_from_excel)} rows\")\n",
    "    print(\"First few rows:\")\n",
    "    print(events_from_excel.head(2))\n",
    "    \n",
    "    # Method 3: Import Excel data to new SQLite database\n",
    "    excel_con = sqlite3.connect('excel_imported_data.db')\n",
    "    \n",
    "    # Import each sheet as a separate table\n",
    "    for sheet_name, data in all_sheets.items():\n",
    "        if sheet_name != 'Summary':  # Skip summary sheet\n",
    "            table_name = f\"excel_{sheet_name.lower()}\"\n",
    "            data.to_sql(table_name, excel_con, if_exists='replace', index=False)\n",
    "            print(f\"✅ Imported {sheet_name} → {table_name} table\")\n",
    "    \n",
    "    excel_con.close()\n",
    "    print(\"\\n🎉 Excel import completed successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Excel file not found. Run the Excel export section first.\")\n",
    "except ImportError:\n",
    "    print(\"❌ Excel reading requires openpyxl library\")\n",
    "    print(\"   Install with: pip install openpyxl\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Excel import failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06610334",
   "metadata": {},
   "source": [
    "## Production-Ready Code Examples\n",
    "\n",
    "### Critical Best Practices for Real Applications\n",
    "\n",
    "When moving from tutorial code to production systems, several important practices ensure your database operations are **secure**, **reliable**, and **maintainable**:\n",
    "\n",
    "#### 🔒 **1. Security Best Practices**\n",
    "- **Parameterized Queries**: Use `?` placeholders instead of string formatting to prevent SQL injection attacks\n",
    "- **Input Validation**: Always validate data types, ranges, and required fields before database operations\n",
    "- **Access Control**: Enable foreign key constraints and use appropriate user permissions\n",
    "\n",
    "#### 🛡️ **2. Error Handling & Reliability**\n",
    "- **Transaction Management**: Use `BEGIN/COMMIT/ROLLBACK` for multi-step operations\n",
    "- **Connection Management**: Properly close connections and handle connection failures\n",
    "- **Exception Handling**: Catch specific database errors and provide meaningful error messages\n",
    "- **Data Integrity**: Validate business rules and data constraints before insertion\n",
    "\n",
    "#### ⚡ **3. Performance Optimization**\n",
    "- **Connection Pooling**: Reuse database connections for multiple operations\n",
    "- **Prepared Statements**: Compile SQL once, execute multiple times efficiently\n",
    "- **Indexes**: Create appropriate indexes for frequently queried columns\n",
    "- **Bulk Operations**: Use batch inserts for large datasets\n",
    "\n",
    "#### 🏗️ **4. Code Architecture**\n",
    "- **Context Managers**: Use `with` statements or custom context managers for automatic cleanup\n",
    "- **Function Separation**: Separate validation, database operations, and business logic\n",
    "- **Configuration**: Store database paths and settings in configuration files\n",
    "- **Logging**: Implement proper logging for debugging and monitoring\n",
    "\n",
    "#### 📊 **5. Data Quality & Validation**\n",
    "- **Schema Validation**: Ensure data matches expected types and constraints\n",
    "- **Range Checking**: Validate that numerical values fall within realistic ranges\n",
    "- **Required Fields**: Check for missing or null values in critical fields\n",
    "- **Referential Integrity**: Ensure foreign key relationships are maintained\n",
    "\n",
    "### What the Following Code Demonstrates:\n",
    "\n",
    "The next cell shows a **production-ready implementation** that incorporates all these best practices:\n",
    "\n",
    "1. **Safe Database Context Manager** - Handles connections, transactions, and cleanup automatically\n",
    "2. **Input Validation Function** - Validates earthquake data before insertion\n",
    "3. **Parameterized Queries** - Prevents SQL injection attacks\n",
    "4. **Error Handling** - Graceful handling of database errors with rollback capability\n",
    "5. **Data Range Validation** - Ensures realistic values for earthquake parameters\n",
    "6. **Foreign Key Enforcement** - Enables database constraints for data integrity\n",
    "\n",
    "This approach transforms simple tutorial code into **best-practice** database operations suitable for real scientific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1faab126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Production-Ready Database Operations:\n",
      "=========================================\n",
      "✅ Safely inserted new event with ID: 2\n"
     ]
    }
   ],
   "source": [
    "# Production-ready database operations\n",
    "print(\"🔧 Production-Ready Database Operations:\")\n",
    "print(\"=\" * 41)\n",
    "\n",
    "def safe_database_operation(db_path, operation_func):\n",
    "    \"\"\"Context manager for safe database operations\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        conn.execute(\"PRAGMA foreign_keys = ON\")  # Enable foreign key constraints\n",
    "        result = operation_func(conn)\n",
    "        conn.commit()\n",
    "        return result\n",
    "    except sqlite3.Error as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print(f\"❌ Database error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def insert_event_safely(conn, event_data):\n",
    "    \"\"\"Safely insert earthquake event with validation\"\"\"\n",
    "    # Validate input data\n",
    "    required_fields = ['event_name', 'magnitude', 'epicentral_latitude', 'epicentral_longitude']\n",
    "    for field in required_fields:\n",
    "        if field not in event_data:\n",
    "            raise ValueError(f\"Missing required field: {field}\")\n",
    "    \n",
    "    # Validate ranges\n",
    "    if not (-90 <= event_data['epicentral_latitude'] <= 90):\n",
    "        raise ValueError(\"Invalid latitude: must be between -90 and 90\")\n",
    "    if not (-180 <= event_data['epicentral_longitude'] <= 180):\n",
    "        raise ValueError(\"Invalid longitude: must be between -180 and 180\")\n",
    "    if not (0 <= event_data['magnitude'] <= 10):\n",
    "        raise ValueError(\"Invalid magnitude: must be between 0 and 10\")\n",
    "    \n",
    "    # Use parameterized query (prevents SQL injection)\n",
    "    query = '''\n",
    "    INSERT INTO event (event_name, magnitude, epicentral_latitude, epicentral_longitude)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "    '''\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query, (\n",
    "        event_data['event_name'],\n",
    "        event_data['magnitude'],\n",
    "        event_data['epicentral_latitude'],\n",
    "        event_data['epicentral_longitude']\n",
    "    ))\n",
    "    \n",
    "    return cursor.lastrowid\n",
    "\n",
    "# Add new event to the database\n",
    "new_event = {\n",
    "    'event_name': 'San Andreas Test',\n",
    "    'magnitude': 5.8,\n",
    "    'epicentral_latitude': 34.0522,\n",
    "    'epicentral_longitude': -118.2437\n",
    "}\n",
    "\n",
    "result = safe_database_operation('gmdatabase.db', \n",
    "                                lambda conn: insert_event_safely(conn, new_event))\n",
    "\n",
    "if result:\n",
    "    print(f\"✅ Safely inserted new event with ID: {result}\")\n",
    "else:\n",
    "    print(\"❌ Failed to insert event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66edfffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 Database connections closed\n",
      "✅ Tutorial completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Clean up: Close the original connection\n",
    "con.close()\n",
    "\n",
    "print(\"\\n🧹 Database connections closed\")\n",
    "print(\"✅ Tutorial completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "IMAGE_NAME": "taccsciapps/ds-nb-img:base-0.1.0",
  "UUID": "7c86762a-be0e-11ed-9d54-cace2d978386",
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
